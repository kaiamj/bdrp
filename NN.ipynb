{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiamj/bdrp/blob/main/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pNFafoLefI5L",
        "outputId": "f1273d2a-66dd-43b9-c9fc-a02ee51394c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/instadeepai/jumanji.git\n",
            "  Cloning https://github.com/instadeepai/jumanji.git to /tmp/pip-req-build-yr5sw4oc\n",
            "  Running command git clone -q https://github.com/instadeepai/jumanji.git /tmp/pip-req-build-yr5sw4oc\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.3) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.3) (4.1.1)\n",
            "Collecting matplotlib>=3.3.4\n",
            "  Downloading matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 28.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.22.0 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.3) (0.25.2)\n",
            "Collecting Pillow>=9.0.0\n",
            "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax>=0.2.26 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.3) (0.3.25)\n",
            "Collecting dm-env>=1.5\n",
            "  Downloading dm_env-1.5-py3-none-any.whl (26 kB)\n",
            "Collecting pygame>=2.0.0\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jaxlib>=0.1.74 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.3) (0.3.25+cuda11.cudnn805)\n",
            "Collecting chex>=0.1.3\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting brax>=0.0.10\n",
            "  Downloading brax-0.0.15-py3-none-any.whl (372 kB)\n",
            "\u001b[K     |████████████████████████████████| 372 kB 69.3 MB/s \n",
            "\u001b[?25hCollecting trimesh\n",
            "  Downloading trimesh-3.17.1-py3-none-any.whl (669 kB)\n",
            "\u001b[K     |████████████████████████████████| 669 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 84.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (6.0)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 81.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (1.50.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (1.3.0)\n",
            "Collecting pytinyrenderer\n",
            "  Downloading pytinyrenderer-0.0.13-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 76.4 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting flax\n",
            "  Downloading flax-0.6.2-py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.3->jumanji==0.1.3) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.3->jumanji==0.1.3) (0.1.7)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.3) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.3) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.3) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.22.0->jumanji==0.1.3) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.2.26->jumanji==0.1.3) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.2.26->jumanji==0.1.3) (1.7.3)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (0.11.0)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
            "\u001b[K     |████████████████████████████████| 295 kB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->jumanji==0.1.3) (1.15.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.3) (1.0.4)\n",
            "Collecting rich>=11.1\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 84.7 MB/s \n",
            "\u001b[?25hCollecting tensorstore\n",
            "  Downloading tensorstore-0.1.28-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=11.1->flax->brax>=0.0.10->jumanji==0.1.3) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX->brax>=0.0.10->jumanji==0.1.3) (3.19.6)\n",
            "Building wheels for collected packages: jumanji\n",
            "  Building wheel for jumanji (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jumanji: filename=jumanji-0.1.3-py3-none-any.whl size=166478 sha256=3cfea286ba57db477320ee1969449402b7c20cf090d18eaa1caed9000c075e02\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y3ctpos7/wheels/df/c6/15/caef8b041b929f4f3b7a55a217c00f24c6931fe57ae40d9bd9\n",
            "Successfully built jumanji\n",
            "Installing collected packages: Pillow, fonttools, contourpy, commonmark, chex, tensorstore, rich, optax, matplotlib, trimesh, tensorboardX, pytinyrenderer, flax, dm-env, dataclasses, pygame, brax, jumanji\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed Pillow-9.3.0 brax-0.0.15 chex-0.1.5 commonmark-0.9.1 contourpy-1.0.6 dataclasses-0.6 dm-env-1.5 flax-0.6.2 fonttools-4.38.0 jumanji-0.1.3 matplotlib-3.6.2 optax-0.1.4 pygame-2.1.2 pytinyrenderer-0.0.13 rich-12.6.0 tensorboardX-2.5.1 tensorstore-0.1.28 trimesh-3.17.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install git+https://github.com/instadeepai/jumanji.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvhuPtb0h_km",
        "outputId": "23133601-0821-4d23-ac67-73a3be5d8b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jumanji\n",
        "from jumanji.wrappers import AutoResetWrapper\n",
        "key = jax.random.PRNGKey(0)\n",
        "env = jumanji.make(\"BinPack-toy-v0\")\n",
        "state, timestep = env.reset(0)\n",
        "# Randomly choose ems_id and item_id using the action mask\n",
        "\n",
        "num_ems, num_items = env.action_spec().num_values\n",
        "ems_item_id = jax.random.choice(\n",
        "    key=key,\n",
        "    a=num_ems * num_items,\n",
        "    p=timestep.observation.action_mask.flatten(),\n",
        ")\n",
        "ems_id, item_id = jnp.divmod(ems_item_id, num_items)\n",
        "\n",
        "#Wrap the action as a jax array of shape (2,)\n",
        "action = jnp.array([ems_id, item_id])\n",
        "state, timestep = env.step(state, action)\n",
        "print(timestep.reward)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import chex\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "QJZD-48Dp-hf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7-JpZNsph_nH"
      },
      "outputs": [],
      "source": [
        "def flatten(obs):\n",
        "  p=[]  # obs = timestep.observation \n",
        "  p = np.append(obs.ems.x1,timestep.observation.ems.x2)\n",
        "  p = np.append(p,obs.ems.x2)\n",
        "  p = np.append(p,obs.ems.y1)\n",
        "  p = np.append(p,obs.ems.y2)\n",
        "  p = np.append(p,obs.ems.z1)\n",
        "  p = np.append(p,obs.ems.z2)\n",
        "  p = np.append(p,obs.ems_mask.flatten())\n",
        "  p = np.append(p,obs.items.x_len)\n",
        "  p = np.append(p,obs.items.y_len)\n",
        "  p = np.append(p,obs.items.z_len)\n",
        "  p = np.append(p,obs.items_mask.flatten())\n",
        "  p = np.append(p,obs.items_placed.flatten())\n",
        "  return p "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = flatten(timestep.observation)"
      ],
      "metadata": {
        "id": "jSHOCZ2nsK-S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE29o1XaxDDw",
        "outputId": "c27e7b6d-80aa-4a01-a31a-ad060e6f3009"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(420,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_8XaiCMh_sB",
        "outputId": "c64efdb1-0944-474e-bbde-60840b63a29d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "jumanji.environments.combinatorial.binpack.types.Observation"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "type(timestep.observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rDAD_BVGi2P0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aef776d-1194-4096-98e9-ec9f7e47aac0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "timestep.observation.action_mask.flatten().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxleMe73i9Eq"
      },
      "outputs": [],
      "source": [
        "env.action_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiLncq7Ji9Hj"
      },
      "outputs": [],
      "source": [
        "env.observation_spec()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p.shape[0]"
      ],
      "metadata": {
        "id": "Lu9eoPWAxU7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39209704-abab-4156-dbdd-9ddce02b6a78"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "420"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = FeedForwardNN(p.shape[0],800)\n",
        "p1 = model1.forward(np.array(p))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYwxRXLbxAWG",
        "outputId": "bc32666f-6cd3-40f2-955e-13c7cb3210bf"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside tensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = FeedForwardNN(p.shape[0],1)\n",
        "critic = model2.forward(np.array(p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Mg-z4uMhPe",
        "outputId": "6e81ea2e-0b7a-4317-ab75-40bffe5eb468"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside tensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "critic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ylCth8dMnmQ",
        "outputId": "4ed625b3-92fe-4d7f-fce4-f3cab297258c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "584U3KB1GH8w",
        "outputId": "f77842b2-48fe-4efc-a2ca-b87454631079"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7.3258e-02, 4.7905e-03, 1.4326e-01, 8.5431e-02, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 1.0771e-01, 2.3255e-02, 9.9250e-02, 7.8145e-02, 7.6100e-02,\n",
              "        0.0000e+00, 2.0643e-01, 0.0000e+00, 6.3144e-02, 7.6659e-02, 0.0000e+00,\n",
              "        0.0000e+00, 5.3319e-02, 2.9052e-01, 0.0000e+00, 1.1991e-01, 2.0213e-01,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0750e-02, 2.5038e-01, 7.0100e-02,\n",
              "        0.0000e+00, 9.5201e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        6.2684e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        7.5674e-02, 2.9135e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9981e-01,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5849e-01, 2.9518e-02, 0.0000e+00,\n",
              "        9.9162e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8625e-01, 1.6340e-01,\n",
              "        0.0000e+00, 0.0000e+00, 1.0939e-01, 2.2355e-02, 0.0000e+00, 1.7597e-01,\n",
              "        1.5238e-01, 4.5492e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5950e-02,\n",
              "        0.0000e+00, 1.7501e-01, 0.0000e+00, 0.0000e+00, 8.0603e-02, 4.0125e-02,\n",
              "        6.3737e-02, 0.0000e+00, 6.1077e-02, 3.6985e-02, 5.1030e-02, 0.0000e+00,\n",
              "        0.0000e+00, 1.1220e-01, 2.3807e-02, 0.0000e+00, 7.2586e-02, 0.0000e+00,\n",
              "        1.7529e-01, 1.5140e-01, 0.0000e+00, 0.0000e+00, 1.9512e-01, 1.2468e-01,\n",
              "        0.0000e+00, 0.0000e+00, 1.6483e-01, 1.4932e-01, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3382e-02, 0.0000e+00, 8.0877e-02,\n",
              "        0.0000e+00, 4.1236e-02, 1.0749e-01, 1.2200e-01, 1.2788e-01, 3.3673e-02,\n",
              "        2.1783e-02, 2.9129e-02, 0.0000e+00, 6.0275e-02, 4.7653e-02, 0.0000e+00,\n",
              "        0.0000e+00, 1.6457e-01, 0.0000e+00, 1.0111e-02, 4.4455e-02, 9.4110e-02,\n",
              "        1.0116e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        5.8917e-02, 0.0000e+00, 3.1736e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        6.4932e-03, 0.0000e+00, 1.9228e-01, 0.0000e+00, 0.0000e+00, 1.4026e-01,\n",
              "        3.4019e-01, 1.8666e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        4.5136e-02, 0.0000e+00, 1.8790e-01, 0.0000e+00, 0.0000e+00, 1.1400e-01,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2471e-01, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 8.7801e-02, 0.0000e+00, 0.0000e+00, 5.1743e-02, 0.0000e+00,\n",
              "        0.0000e+00, 4.5871e-02, 0.0000e+00, 0.0000e+00, 2.7170e-01, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.0444e-01, 0.0000e+00, 2.9800e-01, 0.0000e+00,\n",
              "        3.5324e-02, 3.1838e-01, 0.0000e+00, 0.0000e+00, 8.6942e-02, 1.4047e-01,\n",
              "        2.0110e-01, 0.0000e+00, 6.3064e-02, 0.0000e+00, 7.1566e-02, 1.8246e-01,\n",
              "        2.8024e-02, 4.9901e-02, 0.0000e+00, 1.2505e-01, 8.3604e-02, 0.0000e+00,\n",
              "        0.0000e+00, 1.1894e-01, 2.0768e-01, 0.0000e+00, 7.5244e-02, 1.0876e-01,\n",
              "        3.4256e-03, 1.8252e-01, 0.0000e+00, 2.9881e-01, 2.4593e-02, 0.0000e+00,\n",
              "        0.0000e+00, 7.8324e-02, 5.8408e-02, 5.0792e-02, 1.6910e-02, 0.0000e+00,\n",
              "        0.0000e+00, 1.9679e-01, 1.9465e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        1.8061e-01, 1.8926e-02, 0.0000e+00, 1.6227e-01, 0.0000e+00, 1.7264e-01,\n",
              "        0.0000e+00, 2.9203e-02, 6.8926e-02, 0.0000e+00, 8.5422e-03, 2.2949e-02,\n",
              "        0.0000e+00, 3.1195e-01, 0.0000e+00, 1.3379e-01, 0.0000e+00, 2.2728e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1672e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8442e-02, 1.2903e-01, 7.1433e-02,\n",
              "        1.8443e-03, 0.0000e+00, 4.3648e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 7.2626e-02, 0.0000e+00, 0.0000e+00, 2.3083e-01,\n",
              "        1.2993e-01, 0.0000e+00, 2.2633e-01, 0.0000e+00, 8.6962e-02, 7.3184e-02,\n",
              "        0.0000e+00, 1.4427e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        1.5401e-01, 2.9143e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5783e-01,\n",
              "        0.0000e+00, 0.0000e+00, 7.5754e-02, 5.0369e-02, 0.0000e+00, 0.0000e+00,\n",
              "        1.1640e-01, 0.0000e+00, 7.0190e-02, 1.6056e-01, 0.0000e+00, 0.0000e+00,\n",
              "        1.0497e-01, 0.0000e+00, 3.1495e-02, 1.8827e-01, 0.0000e+00, 0.0000e+00,\n",
              "        8.6098e-02, 2.8396e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6391e-02,\n",
              "        1.3245e-01, 0.0000e+00, 3.0542e-01, 2.7249e-01, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 2.4200e-01, 2.6778e-02, 2.1162e-01, 6.2505e-02, 0.0000e+00,\n",
              "        1.1080e-01, 3.2465e-01, 0.0000e+00, 1.7160e-01, 2.9990e-01, 9.3837e-02,\n",
              "        1.0007e-01, 0.0000e+00, 2.6877e-02, 3.6289e-02, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 3.4033e-02, 8.6817e-02, 1.8553e-01, 1.5542e-01,\n",
              "        0.0000e+00, 1.1517e-01, 4.1060e-02, 5.0827e-02, 1.6084e-01, 1.6591e-02,\n",
              "        9.1157e-02, 1.1145e-01, 1.1843e-01, 0.0000e+00, 1.4200e-01, 1.1804e-01,\n",
              "        0.0000e+00, 1.7920e-01, 0.0000e+00, 8.7536e-02, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 9.1399e-03, 8.1256e-02, 1.0389e-01, 1.2579e-02,\n",
              "        0.0000e+00, 1.2921e-01, 1.5079e-01, 1.8964e-02, 4.2319e-02, 1.0555e-01,\n",
              "        0.0000e+00, 6.2793e-02, 2.1461e-01, 0.0000e+00, 1.3082e-01, 4.3926e-03,\n",
              "        0.0000e+00, 3.8844e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        5.5197e-02, 2.0593e-01, 3.4166e-02, 9.0105e-02, 0.0000e+00, 1.3745e-01,\n",
              "        8.5721e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4358e-01, 1.2555e-01,\n",
              "        0.0000e+00, 2.0912e-02, 0.0000e+00, 4.1233e-02, 0.0000e+00, 7.2857e-02,\n",
              "        1.5837e-01, 1.3474e-01, 0.0000e+00, 4.6186e-02, 0.0000e+00, 1.4205e-01,\n",
              "        0.0000e+00, 0.0000e+00, 1.5886e-01, 7.9320e-02, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 9.3530e-02, 4.7653e-02, 2.0002e-01, 0.0000e+00,\n",
              "        1.5277e-03, 1.9688e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1330e-02, 3.0360e-01,\n",
              "        3.7798e-01, 1.6678e-01, 1.0420e-01, 0.0000e+00, 1.0759e-01, 6.7844e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.0259e-02,\n",
              "        0.0000e+00, 1.6708e-01, 1.6949e-01, 0.0000e+00, 0.0000e+00, 6.9304e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0384e-02, 2.4541e-02, 6.0592e-02,\n",
              "        2.5282e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9950e-01, 0.0000e+00,\n",
              "        1.0340e-01, 0.0000e+00, 6.9776e-02, 3.8276e-02, 0.0000e+00, 2.2499e-01,\n",
              "        1.5245e-01, 9.6857e-02, 0.0000e+00, 0.0000e+00, 1.7139e-01, 1.1108e-01,\n",
              "        1.0143e-01, 1.5482e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3367e-01,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        1.2163e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.3045e-02, 3.0056e-01,\n",
              "        4.8892e-03, 1.3321e-01, 0.0000e+00, 1.9668e-01, 1.6758e-01, 5.5809e-02,\n",
              "        1.2604e-01, 0.0000e+00, 1.3032e-01, 2.3334e-01, 6.5308e-02, 0.0000e+00,\n",
              "        0.0000e+00, 1.0704e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8515e-02,\n",
              "        0.0000e+00, 1.1860e-01, 1.4098e-01, 8.4793e-02, 1.2391e-01, 0.0000e+00,\n",
              "        5.9255e-03, 0.0000e+00, 0.0000e+00, 2.2882e-01, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.6090e-01, 2.0045e-02, 0.0000e+00, 0.0000e+00,\n",
              "        9.6793e-02, 0.0000e+00, 0.0000e+00, 2.8769e-03, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7319e-02, 0.0000e+00, 0.0000e+00,\n",
              "        1.5423e-02, 0.0000e+00, 0.0000e+00, 9.5482e-02, 1.2686e-01, 1.2162e-01,\n",
              "        2.2855e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4043e-01,\n",
              "        0.0000e+00, 0.0000e+00, 1.8370e-01, 0.0000e+00, 2.3223e-01, 1.1874e-01,\n",
              "        2.4447e-01, 5.5636e-02, 1.0506e-01, 2.7339e-01, 2.1754e-01, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 2.5935e-02, 3.8078e-02, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7120e-02, 2.7540e-02, 7.4359e-02,\n",
              "        0.0000e+00, 1.7437e-01, 1.4102e-01, 6.2746e-02, 0.0000e+00, 1.5332e-01,\n",
              "        2.7899e-01, 9.7892e-03, 3.2847e-01, 1.2538e-02, 1.6322e-01, 0.0000e+00,\n",
              "        0.0000e+00, 8.8852e-02, 7.5883e-02, 1.1203e-02, 0.0000e+00, 9.7783e-02,\n",
              "        1.0493e-01, 1.8932e-02, 9.6313e-03, 0.0000e+00, 4.7654e-02, 1.2894e-01,\n",
              "        1.1539e-01, 0.0000e+00, 3.4732e-02, 8.2851e-02, 0.0000e+00, 0.0000e+00,\n",
              "        7.8558e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        7.0187e-02, 2.6166e-01, 6.6577e-02, 0.0000e+00, 2.6948e-02, 0.0000e+00,\n",
              "        0.0000e+00, 6.0165e-02, 1.5199e-01, 1.5778e-01, 0.0000e+00, 0.0000e+00,\n",
              "        6.5575e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8353e-02,\n",
              "        0.0000e+00, 0.0000e+00, 3.3848e-02, 9.0969e-02, 4.2245e-02, 0.0000e+00,\n",
              "        4.0790e-02, 1.0859e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6120e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 1.3251e-01, 0.0000e+00, 1.0410e-01, 2.0023e-02, 2.3464e-01,\n",
              "        0.0000e+00, 0.0000e+00, 2.6517e-01, 0.0000e+00, 0.0000e+00, 5.0078e-02,\n",
              "        4.7843e-02, 0.0000e+00, 2.8998e-02, 0.0000e+00, 0.0000e+00, 6.7291e-02,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7594e-01, 1.4034e-01,\n",
              "        1.5042e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4969e-02,\n",
              "        1.5605e-01, 1.3637e-01, 2.1040e-01, 0.0000e+00, 1.1874e-01, 0.0000e+00,\n",
              "        0.0000e+00, 2.9579e-02, 3.0145e-01, 0.0000e+00, 1.3790e-01, 9.0033e-03,\n",
              "        0.0000e+00, 2.3596e-01, 0.0000e+00, 1.5261e-01, 0.0000e+00, 0.0000e+00,\n",
              "        4.5159e-02, 1.7016e-01, 3.3476e-02, 0.0000e+00, 1.2476e-01, 0.0000e+00,\n",
              "        1.2402e-01, 1.4733e-03, 6.4081e-02, 0.0000e+00, 0.0000e+00, 2.4953e-01,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4809e-02,\n",
              "        1.9794e-01, 2.2739e-01, 1.0539e-01, 2.9463e-02, 5.1585e-02, 0.0000e+00,\n",
              "        9.6193e-02, 6.3660e-02, 1.0169e-01, 1.3626e-01, 1.0052e-01, 1.3569e-01,\n",
              "        0.0000e+00, 1.6802e-01, 3.6672e-02, 0.0000e+00, 1.0280e-01, 0.0000e+00,\n",
              "        1.4101e-01, 4.3347e-02, 9.9461e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        1.2577e-01, 0.0000e+00, 3.9128e-02, 3.9728e-03, 0.0000e+00, 5.0001e-02,\n",
              "        5.9716e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7503e-02, 0.0000e+00,\n",
              "        0.0000e+00, 1.8924e-01, 1.5369e-01, 1.8512e-02, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2445e-02, 1.3171e-01,\n",
              "        7.8877e-03, 1.5435e-01, 0.0000e+00, 0.0000e+00, 1.3752e-01, 1.1842e-02,\n",
              "        2.5057e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2148e-01, 0.0000e+00,\n",
              "        0.0000e+00, 1.0513e-01, 3.0672e-01, 0.0000e+00, 7.2343e-02, 0.0000e+00,\n",
              "        0.0000e+00, 1.4658e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 7.2831e-02, 0.0000e+00, 2.7905e-01, 0.0000e+00, 0.0000e+00,\n",
              "        0.0000e+00, 0.0000e+00, 1.7351e-02, 0.0000e+00, 1.6782e-01, 0.0000e+00,\n",
              "        0.0000e+00, 8.3486e-02, 2.4684e-01, 3.2847e-02, 0.0000e+00, 3.4198e-04,\n",
              "        0.0000e+00, 0.0000e+00], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4ZByF_7PnqX",
        "outputId": "7f4936f0-0f49-4e48-ce48-45639c086614"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([800])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tZCJi6wBGRBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "action_mask = timestep.observation.action_mask.flatten()"
      ],
      "metadata": {
        "id": "pZ_RhOCTQA-x"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_tensor = torch.tensor(np.array(action_mask),dtype=torch.int)"
      ],
      "metadata": {
        "id": "pM-ixupMHRaR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = torch.nonzero(action_tensor)  #getting valid indicies "
      ],
      "metadata": {
        "id": "ZG6-CPJIG01R"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkX5fDQsIXoE",
        "outputId": "2b3f5ca8-443d-4e35-e42a-2e723056e193"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0],\n",
              "        [ 1],\n",
              "        [ 2],\n",
              "        [ 3],\n",
              "        [ 4],\n",
              "        [ 5],\n",
              "        [ 6],\n",
              "        [ 7],\n",
              "        [ 8],\n",
              "        [ 9],\n",
              "        [10],\n",
              "        [12],\n",
              "        [13],\n",
              "        [14],\n",
              "        [15],\n",
              "        [16],\n",
              "        [17],\n",
              "        [18],\n",
              "        [19],\n",
              "        [22],\n",
              "        [23],\n",
              "        [24],\n",
              "        [26],\n",
              "        [28],\n",
              "        [29],\n",
              "        [30],\n",
              "        [32],\n",
              "        [34],\n",
              "        [35],\n",
              "        [36],\n",
              "        [37],\n",
              "        [38],\n",
              "        [39],\n",
              "        [40],\n",
              "        [41],\n",
              "        [42],\n",
              "        [44],\n",
              "        [45],\n",
              "        [47],\n",
              "        [48],\n",
              "        [50],\n",
              "        [52],\n",
              "        [53],\n",
              "        [54],\n",
              "        [57],\n",
              "        [58]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_tensor[indices]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7TeSUp2Iiwp",
        "outputId": "01dcf9f4-88f8-4e8e-fafa-21240f615c5d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1],\n",
              "        [1]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p2 = p1[indices] # getting proper valid action probablities "
      ],
      "metadata": {
        "id": "ZoUQ-0JcJroN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1[indices]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1g4e6hqR4HH",
        "outputId": "f2359a11-2ef5-43bb-c292-33052e0c4c7d"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0733],\n",
              "        [0.0048],\n",
              "        [0.1433],\n",
              "        [0.0854],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.1077],\n",
              "        [0.0233],\n",
              "        [0.0992],\n",
              "        [0.0781],\n",
              "        [0.0000],\n",
              "        [0.2064],\n",
              "        [0.0000],\n",
              "        [0.0631],\n",
              "        [0.0767],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0533],\n",
              "        [0.1199],\n",
              "        [0.2021],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.2504],\n",
              "        [0.0701],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0627],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0757],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.1998],\n",
              "        [0.0000],\n",
              "        [0.0000],\n",
              "        [0.0295],\n",
              "        [0.0000],\n",
              "        [0.0992],\n",
              "        [0.0000],\n",
              "        [0.1862]], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Categorical\n"
      ],
      "metadata": {
        "id": "nCZyH2EXJ1XN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEtz53gZP1Nc",
        "outputId": "1420f4de-9e75-4803-ea70-5b8c81683a24"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([800])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XBXdUY2QN4C",
        "outputId": "1abc109e-0233-4881-b1fb-f11b9bfcba1d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([46, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p2= torch.reshape(p2,(-1,))  # reshapping it to make it 1D"
      ],
      "metadata": {
        "id": "RG5smURRQO9B"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = Categorical( logits = p2) #logits un-normalized probablities \n",
        "a=m.sample() #actions\n",
        "m.log_prob(a) # softmax probablity of that action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gP1lyOmKIiW",
        "outputId": "3de225fe-e070-4ddf-8b4d-12e7c82abf19"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-3.8186, grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_GpxHe3SgvN",
        "outputId": "6226085c-272e-4f2a-cb23-3a4dcb81389d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-3.8081, -3.8765, -3.7381, -3.7959, -3.8813, -3.8813, -3.8813, -3.7736,\n",
              "        -3.8581, -3.7821, -3.8032, -3.8813, -3.6749, -3.8813, -3.8182, -3.8047,\n",
              "        -3.8813, -3.8813, -3.8280, -3.7614, -3.6792, -3.8813, -3.8813, -3.6309,\n",
              "        -3.8112, -3.8813, -3.8813, -3.8813, -3.8813, -3.8186, -3.8813, -3.8813,\n",
              "        -3.8813, -3.8813, -3.8813, -3.8056, -3.8813, -3.8813, -3.6815, -3.8813,\n",
              "        -3.8813, -3.8518, -3.8813, -3.7821, -3.8813, -3.6951],\n",
              "       grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.item() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9JfxUJfKAoI",
        "outputId": "96b346e1-46f2-43b2-947b-b929874be37f"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices[a.item()] # valid action value "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsQRRNozTPlZ",
        "outputId": "e74e7673-082f-4b0d-d575-9e6135907b15"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([36])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # X = torch.tensor([0.1, 0.5, -1.0, 0, 1.2, 0])\n",
        "# mask = action_mask == True\n",
        "# mask\n",
        "# tensor([1, 1, 0, 1, 1, 1], dtype=torch.uint8)\n",
        "\n",
        "# >>> indices = torch.nonzero(mask)\n",
        "# >>> indices\n",
        "# tensor([[0],\n",
        "#         [1],\n",
        "#         [3],\n",
        "#         [4],\n",
        "#         [5]])\n",
        "\n",
        "# >>> X[indices]\n",
        "# tensor([[0.1000],\n",
        "#         [0.5000],\n",
        "#         [0.0000],\n",
        "#         [1.2000],\n",
        "#         [0.0000]])"
      ],
      "metadata": {
        "id": "tenmanW7GWtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ind = action_mask.index[[action_mask[0] == True]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7erGynMqvzu",
        "outputId": "bc6962b3-c1f4-40bf-f69a-196b9efbb102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py:4616: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  result = getitem(key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U4jjcyds5f5",
        "outputId": "c4a64b62-80c2-4a7b-faf3-5d6e74aa3253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 17,\n",
              "            18, 19, 22, 23, 24, 26, 28, 29, 30, 32, 34, 35, 36, 37, 38, 39, 40,\n",
              "            41, 42, 44, 45, 47, 48, 50, 52, 53, 54, 57, 58],\n",
              "           dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(p1.detach().numpy())\n"
      ],
      "metadata": {
        "id": "CVuyauujrpeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[ind][0][4].index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsLHcQX5wiSE",
        "outputId": "f32fa0ae-5d7a-4777-af82-3beb38c2ba4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0.001197\n",
              "1     0.001197\n",
              "2     0.001359\n",
              "3     0.001197\n",
              "4     0.001197\n",
              "5     0.001327\n",
              "6     0.001430\n",
              "7     0.001197\n",
              "8     0.001312\n",
              "9     0.001224\n",
              "10    0.001202\n",
              "12    0.001197\n",
              "13    0.001219\n",
              "14    0.001321\n",
              "15    0.001230\n",
              "16    0.001197\n",
              "17    0.001197\n",
              "18    0.001197\n",
              "19    0.001197\n",
              "22    0.001409\n",
              "23    0.001368\n",
              "24    0.001197\n",
              "26    0.001357\n",
              "28    0.001311\n",
              "29    0.001197\n",
              "30    0.001197\n",
              "32    0.001197\n",
              "34    0.001197\n",
              "35    0.001432\n",
              "36    0.001197\n",
              "37    0.001197\n",
              "38    0.001219\n",
              "39    0.001229\n",
              "40    0.001197\n",
              "41    0.001293\n",
              "42    0.001197\n",
              "44    0.001213\n",
              "45    0.001197\n",
              "47    0.001203\n",
              "48    0.001197\n",
              "50    0.001197\n",
              "52    0.001197\n",
              "53    0.001197\n",
              "54    0.001197\n",
              "57    0.001197\n",
              "58    0.001350\n",
              "Name: 0, dtype: float32"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(df.iloc[ind][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNk4QWNpwK0e",
        "outputId": "fe2d9e47-c9db-4c5e-fe8d-09ddef8b81bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.0013587928842753172,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.0013267110334709287,\n",
              " 0.0014303360367193818,\n",
              " 0.001196884666569531,\n",
              " 0.0013121668016538024,\n",
              " 0.0012236549519002438,\n",
              " 0.0012018720153719187,\n",
              " 0.001196884666569531,\n",
              " 0.0012193493312224746,\n",
              " 0.00132115522865206,\n",
              " 0.0012302374234423041,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.0014090521726757288,\n",
              " 0.0013678219402208924,\n",
              " 0.001196884666569531,\n",
              " 0.0013568168506026268,\n",
              " 0.0013109107967466116,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.0014315623557195067,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.0012186352396383882,\n",
              " 0.0012291870079934597,\n",
              " 0.001196884666569531,\n",
              " 0.0012926310300827026,\n",
              " 0.001196884666569531,\n",
              " 0.00121268630027771,\n",
              " 0.001196884666569531,\n",
              " 0.0012028596829622984,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.001196884666569531,\n",
              " 0.0013501702342182398]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_mask = pd.DataFrame(timestep.observation.action_mask.flatten())\n",
        "ind = action_mask.index[[action_mask[0] == True]]\n",
        "df = pd.DataFrame(p1.detach().numpy())\n",
        "\n",
        "from torch.distributions import Categorical\n",
        "m = Categorical(probs = torch.tensor(list(df.iloc[ind][0])))\n",
        "a = m.sample() "
      ],
      "metadata": {
        "id": "eedoJW9mrrsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir7H3R1SuSmL",
        "outputId": "0c403d3f-82b5-47b4-f760-1e80aaa6fc36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.log_prob(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np462DABtavc",
        "outputId": "93ee5fc0-749b-4468-b3d9-ba7c3063b392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-3.8655)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(timestep.observation.action_mask.flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjcWvU5KUPbZ",
        "outputId": "6089b667-4c5f-413e-8f3c-764cd2a97e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "jaxlib.xla_extension.DeviceArray"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(p1.detach().numpy())\n",
        "df.iloc[list(timestep.observation.action_mask.flatten()),:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1491
        },
        "id": "-1SGz49yPief",
        "outputId": "596e6a6e-e00f-45ee-b029-9a117cdafc43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0\n",
              "0   0.001198\n",
              "1   0.001227\n",
              "2   0.001198\n",
              "3   0.001350\n",
              "4   0.001198\n",
              "5   0.001305\n",
              "6   0.001198\n",
              "7   0.001198\n",
              "8   0.001198\n",
              "9   0.001198\n",
              "10  0.001198\n",
              "12  0.001280\n",
              "13  0.001377\n",
              "14  0.001298\n",
              "15  0.001198\n",
              "16  0.001284\n",
              "17  0.001376\n",
              "18  0.001198\n",
              "19  0.001240\n",
              "22  0.001198\n",
              "23  0.001270\n",
              "24  0.001245\n",
              "26  0.001198\n",
              "28  0.001198\n",
              "29  0.001198\n",
              "30  0.001201\n",
              "32  0.001198\n",
              "34  0.001245\n",
              "35  0.001198\n",
              "36  0.001198\n",
              "37  0.001198\n",
              "38  0.001315\n",
              "39  0.001429\n",
              "40  0.001198\n",
              "41  0.001228\n",
              "42  0.001198\n",
              "44  0.001198\n",
              "45  0.001212\n",
              "47  0.001198\n",
              "48  0.001227\n",
              "50  0.001212\n",
              "52  0.001291\n",
              "53  0.001198\n",
              "54  0.001198\n",
              "57  0.001264\n",
              "58  0.001198"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ee07414-a264-413b-93d5-fc68e151588f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.001350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.001305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.001280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.001377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.001298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.001284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.001376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.001240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.001270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.001245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.001201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.001245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.001315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.001429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.001228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.001212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.001227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.001212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.001291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.001264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.001198</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ee07414-a264-413b-93d5-fc68e151588f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ee07414-a264-413b-93d5-fc68e151588f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ee07414-a264-413b-93d5-fc68e151588f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(timestep.observation.action_mask.flatten())"
      ],
      "metadata": {
        "id": "veXaUzSJXHck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AbMh0J8b1wj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(p1.detach().numpy())\n",
        "df.iloc[list(timestep.observation.action_mask.flatten()),:].idxmax()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxLBPrKq1oNF",
        "outputId": "48e3317a-d139-4a19-e35d-8fdf658cb8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestep.observation.action_mask.flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNHXBJgY114G",
        "outputId": "17ece8a4-d5b9-42f1-eb3c-579fa550157d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "              True,  True,  True, False,  True,  True,  True,  True,\n",
              "              True,  True,  True,  True, False, False,  True,  True,\n",
              "              True, False,  True, False,  True,  True,  True, False,\n",
              "              True, False,  True,  True,  True,  True,  True,  True,\n",
              "              True,  True,  True, False,  True,  True, False,  True,\n",
              "              True, False,  True, False,  True,  True,  True, False,\n",
              "             False,  True,  True, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False],            dtype=bool)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "yQ3vZM0rjE8I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "class FeedForwardNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "    self.layer1 = nn.Linear(in_dim, 64)\n",
        "    self.layer2 = nn.Linear(64, out_dim)\n",
        "    #self.layer3 = nn.Softmax()\n",
        "  def forward(self, obs):\n",
        "  # Convert observation to tensor if it's a numpy array\n",
        "    if isinstance(obs, np.ndarray):\n",
        "      obs = torch.tensor(obs, dtype=torch.float)\n",
        "      print(\"inside tensor\")\n",
        "  \n",
        "    activation1 = F.relu(self.layer1(obs))\n",
        "    output = F.relu(self.layer2(activation1))\n",
        "    #output = self.layer3(activation2)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hZa7hu_jFAG"
      },
      "outputs": [],
      "source": [
        "model1 = FeedForwardNN(timestep.observation.action_mask.flatten().shape[0],800)\n",
        "p = model1.forward(np.array(timestep.observation.action_mask.flatten()))\n",
        "df = pd.DataFrame(p.detach().numpy())\n",
        "df.iloc[list(timestep.observation.action_mask.flatten()),:].idxmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMtNUWDyjxRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7003498c-253d-486d-a998-846731cd1a4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(11, dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "ems_item_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRgJN6816B4u",
        "outputId": "b8b50061-4426-4806-ed00-e6514169ea9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([ 0, 11], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = [ 0.25, 0.25, 0.25, 0.25 ]"
      ],
      "metadata": {
        "id": "J8frhkeQHJBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.distributions import MultivariateNormal,Categorical\n",
        "m = Categorical(torch.tensor(l))\n",
        "m.sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5elpRWLVGp7E",
        "outputId": "6e818732-c1dc-48f8-eeaf-5b181a37eef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtQ9pR7cGy1S",
        "outputId": "67f9e4c1-b593-4adf-ac83-205fb6611a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "F8RSfua-G2us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCbmLLiYjqfB"
      },
      "source": [
        "## PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Xns65TLJjhYD"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import MultivariateNormal,Categorical\n",
        "from torch.optim import Adam\n",
        "class PPO:\n",
        "  def __init__(self,env):\n",
        "    self._init_hyperparameters()\n",
        "    self.env = env\n",
        "    #####################################\n",
        "    self.obs_dim = 420\n",
        "    self.act_dim = 800\n",
        "    ######################################\n",
        "    \n",
        "\n",
        "    #initiate actor and critic\n",
        "    self.actor = FeedForwardNN(self.obs_dim,self.act_dim)\n",
        "    self.critic = FeedForwardNN(self.obs_dim,1)\n",
        "\n",
        "      # Create our variable for the matrix.\n",
        "    # Note that I chose 0.5 for stdev arbitrarily.\n",
        "    self.cov_var = torch.full(size=(self.act_dim,), fill_value=0.5)\n",
        "    \n",
        "    # Create the covariance matrix\n",
        "    self.cov_mat = torch.diag(self.cov_var)\n",
        "    self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
        "    self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
        "    \n",
        "    \n",
        "  def _init_hyperparameters(self):\n",
        "    # Default values for hyperparameters, will need to change later.\n",
        "    self.timesteps_per_batch = 4800            # timesteps per batch\n",
        "    self.max_timesteps_per_episode = 1600      # timesteps per episode\n",
        "    self.gamma = 0.95\n",
        "    self.n_updates_per_iteration = 5\n",
        "    self.clip = 0.2 # As recommended by the paper\n",
        "    self.lr = 0.005\n",
        "\n",
        "  def compute_rtgs(self, batch_rews): \n",
        "    # The rewards-to-go (rtg) per episode per batch to return.\n",
        "    # The shape will be (num timesteps per episode)\n",
        "    batch_rtgs = []\n",
        "    # Iterate through each episode backwards to maintain same order\n",
        "    # in batch_rtgs\n",
        "    for ep_rews in reversed(batch_rews):\n",
        "      discounted_reward = 0 # The discounted reward so far\n",
        "      for rew in reversed(ep_rews):\n",
        "        discounted_reward = rew + discounted_reward * self.gamma\n",
        "        batch_rtgs.insert(0, discounted_reward)\n",
        "    # Convert the rewards-to-go into a tensor\n",
        "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
        "    return batch_rtgs\n",
        "\n",
        "  def get_action(self,obs,action_mask):\n",
        "     p1 = self.actor(obs)\n",
        "     \n",
        "     \n",
        "     ind = action_mask.index[[action_mask[0] == True]]\n",
        "     df = pd.DataFrame(p1.detach().numpy())\n",
        "     probs = Categorical(probs=torch.tensor(list(df.iloc[ind][0])))\n",
        "     action = probs.sample()\n",
        "     action_id = df.iloc[ind][0][action].index\n",
        "     return action_id, action, probs.log_prob(action)#, probs.entropy(), self.critic(obs) \n",
        "    # model1 = FeedForwardNN(obs.shape[0],800)\n",
        "    # p1 = model1.forward(np.array(obs))\n",
        "    # df = pd.DataFrame(p1.detach().numpy())\n",
        "    #df.iloc[list(timestep.observation.action_mask.flatten()),:].idxmax()[0]\n",
        "\n",
        "    #return df.iloc[list(timestep.observation.action_mask.flatten()),:].idxmax()[0]\n",
        "  def rollout(self):\n",
        "    # Batch data\n",
        "    batch_obs = []             # batch observations\n",
        "    batch_acts = []            # batch actions\n",
        "    batch_log_probs = []       # log probs of each action\n",
        "    batch_rews = []            # batch rewards\n",
        "    batch_rtgs = []            # batch rewards-to-go\n",
        "    batch_lens = []            # episodic lengths in batch\n",
        "    # Number of timesteps run so far this batch\n",
        "    t = 0 \n",
        "    while t < self.timesteps_per_batch:\n",
        "      # Rewards this episode\n",
        "      ep_rews = []\n",
        "      key = jax.random.PRNGKey(0)\n",
        "      ###############################\n",
        "      #jax.jit(env.reset)(key)\n",
        "      state, timestep = self.env.reset(key)\n",
        "      ###############################\n",
        "    \n",
        "      for ep_t in range(self.max_timesteps_per_episode):\n",
        "\n",
        "        # Increment timesteps ran this batch so far\n",
        "        t += 1\n",
        "        # Collect observation\n",
        "        ################################################\n",
        "        obs = flatten(timestep.observation)\n",
        "        obs = torch.tensor(obs, dtype=torch.float)\n",
        "        batch_obs.append(obs)\n",
        "        \n",
        "        num_ems, num_items = env.action_spec().num_values\n",
        "        action_mask = pd.DataFrame(timestep.observation.action_mask.flatten())\n",
        "        #----------------------------------------------- get from NN\n",
        "        #ems_item_id = self.get_action(obs,action_mask)\n",
        "        ems_item_id, action_,log_prob  = self.get_action(obs,action_mask)\n",
        "        # -------------------------------------------------\n",
        "        ems_id, item_id = jnp.divmod(ems_item_id, num_items)\n",
        "\n",
        "        # Wrap the action as a jax array of shape (2,)\n",
        "        action = jnp.array([ems_id, item_id])\n",
        "\n",
        "        #action = torch.tensor(action, dtype=torch.float)\n",
        "        #ems_item_id, action_,log_prob  = self.get_action(obs,action_mask)\n",
        "        #mean = self.actor(obs)\n",
        "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
        "        #log_prob = dist.log_prob(action)\n",
        "\n",
        "        state,timestep = self.env.step(action)\n",
        "        rew = timestep.reward\n",
        "        ##################################################\n",
        "        # Collect reward, action, and log prob\n",
        "        ep_rews.append(rew)\n",
        "        \n",
        "        batch_acts.append(action_)\n",
        "        batch_log_probs(log_prob)\n",
        "      # Collect episodic length and rewards\n",
        "      batch_lens.append(ep_t + 1) # plus 1 because timestep starts at 0\n",
        "      batch_rews.append(ep_rews) \n",
        "      # Reshape data as tensors in the shape specified before returning\n",
        "    batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
        "    batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
        "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
        "    # ALG STEP #4\n",
        "    batch_rtgs = self.compute_rtgs(batch_rews)\n",
        "    # Return the batch data\n",
        "    return batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens\n",
        "\n",
        "  def learn(self, total_timesteps):\n",
        "    t_so_far = 0 # Timesteps simulated so far\n",
        "    while t_so_far < total_timesteps:              # ALG STEP 2\n",
        "      # Increment t_so_far somewhere below\n",
        "      # ALG STEP 3\n",
        "      batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
        "      # Calculate how many timesteps we collected this batch   \n",
        "      t_so_far += np.sum(batch_lens)\n",
        "      # Calculate V_{phi, k}\n",
        "      V, _ = self.evaluate(batch_obs, batch_acts)\n",
        "      # ALG STEP 5\n",
        "      # Calculate advantage\n",
        "      A_k = batch_rtgs - V.detach()\n",
        "      # Normalize advantages\n",
        "      A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
        "      for _ in range(self.n_updates_per_iteration):\n",
        "        # Calculate V_phi and pi_theta(a_t | s_t)    \n",
        "        V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
        "        # Calculate ratios\n",
        "        ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
        "        # Calculate surrogate losses\n",
        "        surr1 = ratios * A_k\n",
        "        surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
        "        actor_loss = (-torch.min(surr1, surr2)).mean()\n",
        "        # Calculate gradients and perform backward propagation for actor \n",
        "        # network\n",
        "        self.actor_optim.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optim.step()\n",
        "        critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
        "        # Calculate gradients and perform backward propagation for critic network    \n",
        "        self.critic_optim.zero_grad()    \n",
        "        critic_loss.backward()    \n",
        "        self.critic_optim.step()\n",
        "    \n",
        "  def evaluate(self, batch_obs,batch_acts):\n",
        "    # Query critic network for a value V for each obs in batch_obs.\n",
        "    V = self.critic(batch_obs).squeeze()\n",
        "    # Calculate the log probabilities of batch actions using most \n",
        "    # recent actor network.\n",
        "    # This segment of code is similar to that in get_action()\n",
        "    mean = self.actor(batch_obs)\n",
        "    dist = Categorical(mean)\n",
        "    log_probs = dist.log_prob(batch_acts)\n",
        "    # Return predicted values V and log probs log_probs\n",
        "    return V, log_probs\n",
        " \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "kmqsDt3BjtaN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "77101751-421e-438b-ed7b-955d14f89160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n",
            "<ipython-input-9-a847ad8ec619>:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.layer3(activation2)\n",
            "/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py:4616: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  result = getitem(key)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: tensor(8)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-538d8c81b35e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-09b238fd39de>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m    136\u001b[0m       \u001b[0;31m# Increment t_so_far somewhere below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0;31m# ALG STEP 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m       \u001b[0mbatch_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_acts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rtgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m       \u001b[0;31m# Calculate how many timesteps we collected this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0mt_so_far\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-09b238fd39de>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m#----------------------------------------------- get from NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m#ems_item_id = self.get_action(obs,action_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mems_item_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_prob\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;31m# -------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mems_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mems_item_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-09b238fd39de>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs, action_mask)\u001b[0m\n\u001b[1;32m     57\u001b[0m      \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m      \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m      \u001b[0maction_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0maction_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, probs.entropy(), self.critic(obs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# model1 = FeedForwardNN(obs.shape[0],800)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: tensor(8)"
          ]
        }
      ],
      "source": [
        "model = PPO(env)\n",
        "model.learn(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(action, np.ndarray):\n",
        "  action = torch.tensor(action, dtype=torch.float)\n",
        "  print(\"inside action tensor\")\n",
        "else:\n",
        "  print(\"no\")"
      ],
      "metadata": {
        "id": "Tx-5h_fG_v2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KtT_Hk3QJ2q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action = torch.tensor(action)"
      ],
      "metadata": {
        "id": "QA7aC7TWHB5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "b1Q_rBApHuFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "action = tf.convert_to_tensor(action, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "EpLoQSg1HR1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "slk-5v50HYbk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhqxf4uv53aLV8bJoJNQ7y",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}