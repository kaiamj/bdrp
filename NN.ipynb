{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiamj/bdrp/blob/main/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNFafoLefI5L",
        "outputId": "7375e4c5-8ac7-4ccc-fa03-83b880687614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/instadeepai/jumanji.git\n",
            "  Cloning https://github.com/instadeepai/jumanji.git to /tmp/pip-req-build-xyqbr57s\n",
            "  Running command git clone -q https://github.com/instadeepai/jumanji.git /tmp/pip-req-build-xyqbr57s\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (0.25.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (3.5.3)\n",
            "Requirement already satisfied: brax>=0.0.10 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (0.0.15)\n",
            "Requirement already satisfied: dm-env>=1.5 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (1.5)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (4.1.1)\n",
            "Requirement already satisfied: chex>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (0.1.5)\n",
            "Requirement already satisfied: jaxlib>=0.1.74 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (0.3.25+cuda11.cudnn805)\n",
            "Requirement already satisfied: Pillow>=9.0.0 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (9.3.0)\n",
            "Requirement already satisfied: pygame>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (2.1.2)\n",
            "Requirement already satisfied: jax>=0.2.26 in /usr/local/lib/python3.7/dist-packages (from jumanji==0.1.3) (0.3.25)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (0.6)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (0.1.4)\n",
            "Requirement already satisfied: pytinyrenderer in /usr/local/lib/python3.7/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (0.0.13)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.7/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (6.0)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.7/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (1.50.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (1.3.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (2.5.1)\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.7/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (3.16.4)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (from brax>=0.0.10->jumanji==0.1.3) (0.6.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.1.3->jumanji==0.1.3) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.1.3->jumanji==0.1.3) (0.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.22.0->jumanji==0.1.3) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.22.0->jumanji==0.1.3) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.22.0->jumanji==0.1.3) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.22.0->jumanji==0.1.3) (3.10.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.26->jumanji==0.1.3) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.26->jumanji==0.1.3) (3.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (4.38.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.3) (21.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->jumanji==0.1.3) (1.15.0)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.7/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.3) (0.1.28)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.7/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.3) (12.6.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.3) (1.0.4)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich>=11.1->flax->brax>=0.0.10->jumanji==0.1.3) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=11.1->flax->brax>=0.0.10->jumanji==0.1.3) (2.6.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->brax>=0.0.10->jumanji==0.1.3) (3.19.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/instadeepai/jumanji.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvhuPtb0h_km",
        "outputId": "db86deef-f257-48d7-9f8a-385888c1d70e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/ops/scatter.py:90: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/ops/scatter.py:90: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/ops/scatter.py:90: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/ops/scatter.py:90: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/ops/scatter.py:90: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/ops/scatter.py:90: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jumanji\n",
        "from jumanji.wrappers import AutoResetWrapper\n",
        "key = jax.random.PRNGKey(0)\n",
        "env = jumanji.make(\"BinPack-toy-v0\")\n",
        "state, timestep = env.reset(0)\n",
        "# Randomly choose ems_id and item_id using the action mask\n",
        "\n",
        "num_ems, num_items = env.action_spec().num_values\n",
        "ems_item_id = jax.random.choice(\n",
        "    key=key,\n",
        "    a=num_ems * num_items,\n",
        "    p=timestep.observation.action_mask.flatten(),\n",
        ")\n",
        "ems_id, item_id = jnp.divmod(ems_item_id, num_items)\n",
        "\n",
        "#Wrap the action as a jax array of shape (2,)\n",
        "action = jnp.array([ems_id, item_id])\n",
        "state, timestep = env.step(state, action)\n",
        "print(timestep.reward)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import chex\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "QJZD-48Dp-hf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = jnp.empty((1700,)"
      ],
      "metadata": {
        "id": "ETFIYfoBvPJx"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timestep.observation.ems.x1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx55hM-PyyiZ",
        "outputId": "40835dcd-f5d1-4073-d697-5bfcdbf3a910"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40,)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.concatenate(timestep.observation.ems.x2,timestep.observation.ems.x1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "MVd_bCiSumbv",
        "outputId": "960a76a7-d15c-4770-9685-8e8328ff7ce4"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-2644fe321fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(arrays, axis, dtype)\u001b[0m\n\u001b[1;32m   1769\u001b[0m                 axis: Optional[int] = 0, dtype: Optional[DTypeLike] = None) -> Array:\n\u001b[1;32m   1770\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1771\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_concatenate_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1772\u001b[0m   \u001b[0m_stackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_check_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"concatenate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_concatenate_array\u001b[0;34m(arr, axis, dtype)\u001b[0m\n\u001b[1;32m   1759\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Zero-dimensional arrays cannot be concatenated.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_canonicalize_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Zero-dimensional arrays cannot be concatenated."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF8ByydGxdkd",
        "outputId": "712c43e1-aa3b-4c09-ff50-abbdd442d6eb"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[0.],\n",
              "             [0.],\n",
              "             [0.],\n",
              "             ...,\n",
              "             [0.],\n",
              "             [0.],\n",
              "             [0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.numpy.append(arr,timestep.observation.ems.x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgCC4iZGxZea",
        "outputId": "f91a47c6-dfc0-4837-a4fa-980f070b4d78"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chex.Array == True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suWBGotltRx1",
        "outputId": "4818519f-1b77-41a6-967a-42c84224e5f2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "p = np.append(timestep.observation.ems.x1,timestep.observation.ems.x2)\n"
      ],
      "metadata": {
        "id": "FvV9R_Hvz9o6"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0rXBC0k0JrS",
        "outputId": "28a25bdf-04e4-4b4b-994a-d2cba308859b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.47478703, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.99999994, 0.99999994, 0.99999994, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "7-JpZNsph_nH"
      },
      "outputs": [],
      "source": [
        "def flatten(obs):\n",
        "  p=[]  # obs = timestep.observation \n",
        "  p = np.append(obs.ems.x1,timestep.observation.ems.x2)\n",
        "  p = np.append(p,obs.ems.x2)\n",
        "  p = np.append(p,obs.ems.y1)\n",
        "  p = np.append(p,obs.ems.y2)\n",
        "  p = np.append(p,obs.ems.z1)\n",
        "  p = np.append(p,obs.ems.z2)\n",
        "  p = np.append(p,obs.ems_mask.flatten())\n",
        "  p = np.append(p,obs.items.x_len)\n",
        "  p = np.append(p,obs.items.y_len)\n",
        "  p = np.append(p,obs.items.z_len)\n",
        "  p = np.append(p,obs.items_mask.flatten())\n",
        "  p = np.append(p,obs.items_placed.flatten())\n",
        "  return p "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSHOCZ2nsK-S",
        "outputId": "bca095ab-a281-4f48-a0a4-0acf3993acde"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.47478703, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.99999994, 0.99999994, 0.99999994, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.99999994, 0.99999994, 0.99999994, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.4416309 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       1.        , 1.        , 1.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.3181818 , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       1.        , 1.        , 1.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       1.        , 1.        , 1.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.41652468, 0.52521294, 0.3321976 , 0.5834753 , 0.52521294,\n",
              "       0.47478703, 0.5834753 , 0.47478703, 0.47478703, 0.5834753 ,\n",
              "       0.52521294, 0.47478703, 0.22061327, 0.47478703, 0.52521294,\n",
              "       0.5834753 , 0.5834753 , 0.14258943, 0.19591141, 0.5834753 ,\n",
              "       0.56051505, 0.61330473, 0.5583691 , 0.13776824, 0.5       ,\n",
              "       1.        , 0.1248927 , 0.64549357, 0.35450643, 0.2       ,\n",
              "       0.3866953 , 0.4416309 , 0.43948498, 1.        , 0.5       ,\n",
              "       0.2       , 0.28454936, 0.5583691 , 0.43948498, 0.2527897 ,\n",
              "       0.46454546, 0.24954545, 0.3181818 , 0.46454546, 0.2859091 ,\n",
              "       0.09090909, 0.46454546, 0.07136364, 0.07136364, 0.165     ,\n",
              "       0.24954545, 0.3181818 , 0.46454546, 0.055     , 0.2859091 ,\n",
              "       0.29954547, 0.46454546, 0.3181818 , 0.46454546, 0.46454546,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 1.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE29o1XaxDDw",
        "outputId": "b6535330-3e27-4d0a-ae2f-b56dd294eadd"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(420,)"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_8XaiCMh_sB",
        "outputId": "7378e0c9-df01-4f2c-f978-b93fdade7734"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "jumanji.environments.combinatorial.binpack.types.Observation"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(timestep.observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDAD_BVGi2P0"
      },
      "outputs": [],
      "source": [
        "timestep.observation.action_mask.flatten().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxleMe73i9Eq"
      },
      "outputs": [],
      "source": [
        "env.action_spec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiLncq7Ji9Hj"
      },
      "outputs": [],
      "source": [
        "env.observation_spec()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lu9eoPWAxU7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = FeedForwardNN(p.shape[0],800)\n",
        "p1 = model1.forward(np.array(p))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYwxRXLbxAWG",
        "outputId": "cc54116a-1837-40d3-901e-fc7f6458055d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AbMh0J8b1wj_"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(p1.detach().numpy())\n",
        "df.iloc[list(timestep.observation.action_mask.flatten()),:].idxmax()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxLBPrKq1oNF",
        "outputId": "48e3317a-d139-4a19-e35d-8fdf658cb8a4"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestep.observation.action_mask.flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNHXBJgY114G",
        "outputId": "c0f0863e-4294-4532-812a-3e172aaf8d45"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "              True,  True,  True, False,  True,  True,  True,  True,\n",
              "              True,  True,  True,  True, False, False,  True,  True,\n",
              "              True, False,  True, False,  True,  True,  True, False,\n",
              "              True, False,  True,  True,  True,  True,  True,  True,\n",
              "              True,  True,  True, False,  True,  True, False,  True,\n",
              "              True, False,  True, False,  True,  True,  True, False,\n",
              "             False,  True,  True, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False,\n",
              "             False, False, False, False, False, False, False, False],            dtype=bool)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "yQ3vZM0rjE8I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "class FeedForwardNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "    self.layer1 = nn.Linear(in_dim, 64)\n",
        "    self.layer2 = nn.Linear(64, out_dim)\n",
        "    self.layer3 = nn.Softmax()\n",
        "  def forward(self, obs):\n",
        "  # Convert observation to tensor if it's a numpy array\n",
        "    if isinstance(obs, np.ndarray):\n",
        "      obs = torch.tensor(obs, dtype=torch.float)\n",
        "      print(\"inside tensor\")\n",
        "  \n",
        "    activation1 = F.relu(self.layer1(obs))\n",
        "    activation2 = F.relu(self.layer2(activation1))\n",
        "    output = self.layer3(activation2)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hZa7hu_jFAG"
      },
      "outputs": [],
      "source": [
        "model1 = FeedForwardNN(timestep.observation.action_mask.flatten().shape[0],800)\n",
        "p = model1.forward(np.array(timestep.observation.action_mask.flatten()))\n",
        "df = pd.DataFrame(p.detach().numpy())\n",
        "df.iloc[list(timestep.observation.action_mask.flatten()),:].idxmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "iMtNUWDyjxRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7003498c-253d-486d-a998-846731cd1a4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(11, dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "ems_item_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRgJN6816B4u",
        "outputId": "3655ce43-1cd1-4638-883e-d74f2d32d866"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(420,)"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCbmLLiYjqfB"
      },
      "source": [
        "## PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "Xns65TLJjhYD"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import MultivariateNormal\n",
        "from torch.optim import Adam\n",
        "class PPO:\n",
        "  def __init__(self,env):\n",
        "    self._init_hyperparameters()\n",
        "    self.env = env\n",
        "    #####################################\n",
        "    self.obs_dim = 420\n",
        "    self.act_dim = 800\n",
        "    ######################################\n",
        "    \n",
        "\n",
        "    #initiate actor and critic\n",
        "    self.actor = FeedForwardNN(self.obs_dim,self.act_dim)\n",
        "    self.critic = FeedForwardNN(self.obs_dim,1)\n",
        "\n",
        "      # Create our variable for the matrix.\n",
        "    # Note that I chose 0.5 for stdev arbitrarily.\n",
        "    self.cov_var = torch.full(size=(self.act_dim,), fill_value=0.5)\n",
        "    \n",
        "    # Create the covariance matrix\n",
        "    self.cov_mat = torch.diag(self.cov_var)\n",
        "    self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
        "    self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
        "    \n",
        "    \n",
        "  def _init_hyperparameters(self):\n",
        "    # Default values for hyperparameters, will need to change later.\n",
        "    self.timesteps_per_batch = 4800            # timesteps per batch\n",
        "    self.max_timesteps_per_episode = 1600      # timesteps per episode\n",
        "    self.gamma = 0.95\n",
        "    self.n_updates_per_iteration = 5\n",
        "    self.clip = 0.2 # As recommended by the paper\n",
        "    self.lr = 0.005\n",
        "\n",
        "  def compute_rtgs(self, batch_rews): \n",
        "    # The rewards-to-go (rtg) per episode per batch to return.\n",
        "    # The shape will be (num timesteps per episode)\n",
        "    batch_rtgs = []\n",
        "    # Iterate through each episode backwards to maintain same order\n",
        "    # in batch_rtgs\n",
        "    for ep_rews in reversed(batch_rews):\n",
        "      discounted_reward = 0 # The discounted reward so far\n",
        "      for rew in reversed(ep_rews):\n",
        "        discounted_reward = rew + discounted_reward * self.gamma\n",
        "        batch_rtgs.insert(0, discounted_reward)\n",
        "    # Convert the rewards-to-go into a tensor\n",
        "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
        "    return batch_rtgs\n",
        "\n",
        "  def get_action(self,obs):\n",
        "    model1 = FeedForwardNN(obs.shape[0],800)\n",
        "    p1 = model1.forward(np.array(obs))\n",
        "    df = pd.DataFrame(p1.detach().numpy())\n",
        "    #df.iloc[list(timestep.observation.action_mask.flatten()),:].idxmax()[0]\n",
        "\n",
        "    return df.iloc[list(timestep.observation.action_mask.flatten()),:].idxmax()[0]\n",
        "  def rollout(self):\n",
        "    # Batch data\n",
        "    batch_obs = []             # batch observations\n",
        "    batch_acts = []            # batch actions\n",
        "    batch_log_probs = []       # log probs of each action\n",
        "    batch_rews = []            # batch rewards\n",
        "    batch_rtgs = []            # batch rewards-to-go\n",
        "    batch_lens = []            # episodic lengths in batch\n",
        "    # Number of timesteps run so far this batch\n",
        "    t = 0 \n",
        "    while t < self.timesteps_per_batch:\n",
        "      # Rewards this episode\n",
        "      ep_rews = []\n",
        "      key = jax.random.PRNGKey(0)\n",
        "      ###############################\n",
        "      #jax.jit(env.reset)(key)\n",
        "      state, timestep = self.env.reset(key)\n",
        "      ###############################\n",
        "    \n",
        "      for ep_t in range(self.max_timesteps_per_episode):\n",
        "\n",
        "        # Increment timesteps ran this batch so far\n",
        "        t += 1\n",
        "        # Collect observation\n",
        "        ################################################\n",
        "        obs = flatten(timestep.observation)\n",
        "        batch_obs.append(obs)\n",
        "        \n",
        "        num_ems, num_items = env.action_spec().num_values\n",
        "        #----------------------------------------------- get from NN\n",
        "        ems_item_id = self.get_action(obs)\n",
        "        # -------------------------------------------------\n",
        "        ems_id, item_id = jnp.divmod(ems_item_id, num_items)\n",
        "\n",
        "        # Wrap the action as a jax array of shape (2,)\n",
        "        action = jnp.array([ems_id, item_id])\n",
        "        mean = self.actor(obs)\n",
        "        dist = MultivariateNormal(mean, self.cov_mat)\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        state,timestep = self.env.step(action)\n",
        "        rew = timestep.reward\n",
        "        ##################################################\n",
        "        # Collect reward, action, and log prob\n",
        "        ep_rews.append(rew)\n",
        "        batch_acts.append(action)\n",
        "        batch_log_probs(log_prob.detach())\n",
        "      # Collect episodic length and rewards\n",
        "      batch_lens.append(ep_t + 1) # plus 1 because timestep starts at 0\n",
        "      batch_rews.append(ep_rews) \n",
        "      # Reshape data as tensors in the shape specified before returning\n",
        "    batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
        "    batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
        "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
        "    # ALG STEP #4\n",
        "    batch_rtgs = self.compute_rtgs(batch_rews)\n",
        "    # Return the batch data\n",
        "    return batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens\n",
        "\n",
        "  def learn(self, total_timesteps):\n",
        "    t_so_far = 0 # Timesteps simulated so far\n",
        "    while t_so_far < total_timesteps:              # ALG STEP 2\n",
        "      # Increment t_so_far somewhere below\n",
        "      # ALG STEP 3\n",
        "      batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
        "      # Calculate how many timesteps we collected this batch   \n",
        "      t_so_far += np.sum(batch_lens)\n",
        "      # Calculate V_{phi, k}\n",
        "      V, _ = self.evaluate(batch_obs, batch_acts)\n",
        "      # ALG STEP 5\n",
        "      # Calculate advantage\n",
        "      A_k = batch_rtgs - V.detach()\n",
        "      # Normalize advantages\n",
        "      A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
        "      for _ in range(self.n_updates_per_iteration):\n",
        "        # Calculate V_phi and pi_theta(a_t | s_t)    \n",
        "        V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
        "        # Calculate ratios\n",
        "        ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
        "        # Calculate surrogate losses\n",
        "        surr1 = ratios * A_k\n",
        "        surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
        "        actor_loss = (-torch.min(surr1, surr2)).mean()\n",
        "        # Calculate gradients and perform backward propagation for actor \n",
        "        # network\n",
        "        self.actor_optim.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optim.step()\n",
        "        critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
        "        # Calculate gradients and perform backward propagation for critic network    \n",
        "        self.critic_optim.zero_grad()    \n",
        "        critic_loss.backward()    \n",
        "        self.critic_optim.step()\n",
        "    \n",
        "  def evaluate(self, batch_obs,batch_acts):\n",
        "    # Query critic network for a value V for each obs in batch_obs.\n",
        "    V = self.critic(batch_obs).squeeze()\n",
        "    # Calculate the log probabilities of batch actions using most \n",
        "    # recent actor network.\n",
        "    # This segment of code is similar to that in get_action()\n",
        "    mean = self.actor(batch_obs)\n",
        "    dist = MultivariateNormal(mean, self.cov_mat)\n",
        "    log_probs = dist.log_prob(batch_acts)\n",
        "    # Return predicted values V and log probs log_probs\n",
        "    return V, log_probs\n",
        " \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "kmqsDt3BjtaN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "ae409d2f-d838-4300-e9b5-41f6f7db6864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside tensor\n",
            "inside tensor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-147-538d8c81b35e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-146-5508d2897bd2>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0;31m# Increment t_so_far somewhere below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0;31m# ALG STEP 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m       \u001b[0mbatch_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_acts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rtgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m       \u001b[0;31m# Calculate how many timesteps we collected this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0mt_so_far\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-146-5508d2897bd2>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimestep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument to log_prob must be a Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mevent_dim_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The value argument to log_prob must be a Tensor"
          ]
        }
      ],
      "source": [
        "model = PPO(env)\n",
        "model.learn(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tx-5h_fG_v2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVSsiAEaDmz0Ltq45+OGWY",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}